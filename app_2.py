# -*- coding: utf-8 -*-
"""app_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WJd9TjauT6mwDuq2qj_CUm3L88ybQt-L
"""

import streamlit as st
import pandas as pd
from io import StringIO
from textblob import TextBlob
from PIL import Image

import nltk, re
from nltk.corpus import stopwords
from nltk.tokenize.toktok import ToktokTokenizer
tokenizer = ToktokTokenizer()
from nltk import word_tokenize
nltk.download('punkt', force=True)
#nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

from transformers import pipeline

# Streamlit app
def main():
    st.title("SurveySage - Part 2")

    st.sidebar.header("SurveySage")
    # Display the logo
    logo_path = "/Users/daviesha/Desktop/DATA 5420/Homework/Final/Streamlit_App/icon.png"
    logo = Image.open(logo_path)
    st.sidebar.image(logo, use_container_width = True)

    st.sidebar.header("Upload Your CSV File")
    uploaded_file = st.sidebar.file_uploader("Upload a CSV file", type=["csv"])

    if uploaded_file is not None:
        # Read CSV
        try:
            df = pd.read_csv(uploaded_file)
        except Exception as e:
            st.error(f"Error reading CSV: {e}")
            return

        st.subheader("Uploaded CSV")
        st.write(df.head())

        # Sentiment Analysis
        st.subheader("Categorized CSV")

        ### Text Preprocessing ###
        # Make a copy of dataframe for edits
        df_edit = df.copy()

        # Add in custom Stop words
        stop_words = nltk.corpus.stopwords.words('english')
        stop_words = stop_words + ['freshmen', 'freshman', 'academy', 'school', 'course', 'class', 'program', 'huntsman', 'business']

        # remove "not" from stop word list because important for analysis
        if 'not' in stop_words:
          stop_words.remove('not')

        # Preprocessing Columns (Suggestions, Fav_Session, Team_Connection, Huntsman_Sentiment, Course_Experience)
          # Sentiment Analysis = Course_Experience, Team_Connection
          # Topic Modeling = Suggestions, Fav_Session, Huntsman Sentiment

        # Sentiment Analysis Preprocessing
        def preprocess_text_sa(text):
            if not isinstance(text, str):
              return ""
            text = re.sub(r"[^a-zA-Z]", " ", text.lower()) #removing everything except for letters
            tokens = nltk.word_tokenize(text)
            tokens = [word for word in tokens if word not in stop_words]
            text = ' '.join(tokens)
            # not doing lemmatization because by taking off prefixes could lose critical info
            return text

        # Apply preprocessing to columns
        df_edit['Course_Experience_clean'] = df_edit['Course_Experience'].apply(preprocess_text_sa)
        df_edit['Team_Connection_clean'] = df_edit['Team_Connection'].apply(preprocess_text_sa)


        ### Sentiment Analysis ###
        sentiment_pipeline = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english") # , device=-1

        # Apply the sentiment analysis pipeline to each course review
        df_edit['Course_Sentiment'] = df_edit['Course_Experience_clean'].apply(sentiment_pipeline).apply(lambda x: x[0]['label']).str.title()

        # Apply the sentiment analysis pipeline to each Team/Peer Coach review
        df_edit['Team_Sentiment'] = df_edit['Team_Connection_clean'].apply(sentiment_pipeline).apply(lambda x: x[0]['label']).str.title()


        ### Final Edits to csv ###
        # drop columns
        columns_to_drop = ['Course_Experience_clean', 'Team_Connection_clean']
        df_edit = df_edit.drop(columns=[col for col in columns_to_drop if col in df_edit.columns])

        # Rearrange columns
        cols_arrange = ['Course_Experience', 'Course_Sentiment', 'Team_Connection', 'Team_Sentiment']
        df_sub = df_edit[cols_arrange]
        df_edit = df_edit.drop(columns=[col for col in cols_arrange if col in df_edit.columns])
        df_edit = pd.concat([df_edit, df_sub], axis=1)

        st.write(df_edit.head())

        # Download Main Categorized CSV
        st.download_button(
            label="Download Categorized CSV",
            data=convert_df_to_csv(df_edit),
            file_name="categorized_data.csv",
            mime="text/csv"
        )

# Convert DataFrame to CSV for download
def convert_df_to_csv(df):
    buffer = StringIO()
    df.to_csv(buffer, index=False)
    buffer.seek(0)
    return buffer.getvalue()

# Run the app
if __name__ == "__main__":
    main()